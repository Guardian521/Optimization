分别使用了sklearn中封装好的svm算法（采用smo进行优化），和按照sgd和admm进行优化的svm算法。

乳腺癌数据集（load_breast_cancer）是scikit-learn库中的一个内置数据集，由威斯康星大学麦迪逊分校提供。该数据集包含了569个样本和30个特征，用于预测肿瘤是良性还是恶性。特征包括细胞核的平均半径、质地、周长、面积、平滑度、紧致度、凹度、凹点、对称性和分形维度等。每个特征都有三个统计量（均值、标准误差、最大值），因此总共有30个特征。

在进行SVM分类任务之前，需要对数据进行预处理。首先，使用sklearn.datasets中的load_breast_cancer()函数加载数据集。然后，使用sklearn.model_selection中的train_test_split()函数将数据集分割成训练集和测试集。接下来，使用sklearn.preprocessing中的StandardScaler()类对数据进行标准化处理，使得数据均值为0，方差为1。最后，使用torch.tensor()函数将数据转换为PyTorch的张量（Tensor）格式。

在SVM模型的实现中，使用了PyTorch中的torch.nn.Module类和nn.Linear()函数定义了一个支持向量机（SVM）模型。线性层用于构建简单的线性模型。在初始化SVM模型时，通过SVM(n_features)指定了输入数据的特征数量。

在核函数的选择上，采用了多项式核函数。核SVM算法的目标函数是通过引入核函数来处理非线性可分问题。核函数能够将原始特征空间映射到更高维的空间，使得在新空间中线性可分。

ADMM算法是一种用于解决凸优化问题的算法，它通过引入辅助变量和拉格朗日乘子来分解原问题，使其更容易求解。在ADMM算法中，初始参数包括决策变量X0、辅助变量Z0和拉格朗日乘子U0。惩罚参数ρ控制拉格朗日乘子更新步长，学习率α控制参数更新步长，正则化参数λ用于控制模型复杂度。

在损失函数的选择上，采用了hinge loss和squared hinge loss。这两种损失函数都是用于SVM分类的常用损失函数。hinge loss在较大的错误上受到的惩罚更大，而在较小的错误受到的惩罚略轻。squared hinge loss则是hinge loss的平方版本，它对较大的误差（异常值）更敏感。

在SGD算法的实现中，选择了学习率为0.01，正则项系数λ为0.05。SGD是一种用于优化损失函数的迭代方法，通过逐步更新模型参数以最小化损失。

梯度裁剪是一种在训练深度神经网络时常用的技巧，用于防止梯度爆炸问题。在训练过程中，如果梯度的总范数超过max_norm，梯度将按比例缩小，使其范数等于max_norm。

经典SVM（SVC）使用了sklearn库中的svm包，该模型来自libsvm。优化算法使用的是基于序列最小优化（Sequential Minimal Optimization, SMO）算法的优化策略。SMO算法通过将大规模优化问题分解为一系列的小规模二次优化问题来进行求解，极大地提高了计算效率和内存使用效率。

在结果比对中，SVM_ADMM和SVM_SGD分别在第11和第5轮达到收敛，而经典SVM（SVC）在第1989轮次模型达到收敛。在训练时间、accuracy和收敛轮次的对比中，SMO算法在训练SVM方面的高效性主要来自其独特的优化策略，每次迭代只处理两个变量，这使得其每次迭代的计算开销非常小，收敛速度较快。在许多实际应用中，这种方法比ADMM和SGD更适合用于训练支持向量机模型。
